{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import concurrent.futures\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import threading\n",
    "from collections import Counter\n",
    "\n",
    "# ====================== é…ç½®éƒ¨åˆ† ======================\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s [%(levelname)s] %(message)s')\n",
    "\n",
    "# ====================== ç½‘ç»œè¿æ¥é…ç½® ======================\n",
    "def create_session(retries=3):\n",
    "    \"\"\"åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„ä¼šè¯å¯¹è±¡\"\"\"\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        backoff_factor=0.3,\n",
    "        status_forcelist=(500, 502, 504)\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry,pool_connections=50,pool_maxsize=100)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "# ====================== å·¥å…·å‡½æ•° ======================\n",
    "def format_url_date(date_obj):\n",
    "    \"\"\"ç”Ÿæˆæ— å‰å¯¼é›¶çš„æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆè·¨å¹³å°å…¼å®¹ï¼‰\"\"\"\n",
    "    return f\"{date_obj.year}-{date_obj.month}-{date_obj.day}\"\n",
    "\n",
    "def parse_date_from_url(url):\n",
    "    \"\"\"ä»URLè§£ææ—¥æœŸ\"\"\"\n",
    "    path = urlparse(url).path\n",
    "    date_part = path.split('/')[-1].replace('.html', '')\n",
    "    return datetime.strptime(date_part, \"%Y-%m-%d\").strftime(\"%Y/%m/%d 00:00:00\")\n",
    "\n",
    "# ====================== æ•°æ®æŠ“å–å‡½æ•° ======================\n",
    "def extract_suitable_items(soup):\n",
    "    \"\"\"æå–å®œ/å¿Œä¿¡æ¯\"\"\"\n",
    "    result = {\"yi\": \"\", \"ji\": \"\"}\n",
    "    for suitable_div in soup.find_all('div', class_='suitable'):\n",
    "        span = suitable_div.find('span')\n",
    "        if not span: continue\n",
    "        category = span.text.strip()\n",
    "        if category not in [\"å®œ\", \"å¿Œ\"]: continue\n",
    "        content_div = suitable_div.find_next_sibling('div', class_='suitable_con')\n",
    "        if not content_div: continue\n",
    "        items = [li.span.text.strip() for li in content_div.find_all('li')]\n",
    "        result[\"yi\" if category == \"å®œ\" else \"ji\"] = ' '.join(items)\n",
    "    return result\n",
    "\n",
    "def extract_lunar_info(soup):\n",
    "    \"\"\"æå–å†œå†/å¹²æ”¯/ç”Ÿè‚–/å½­ç¥–ç™¾å¿Œ\"\"\"\n",
    "    lunar_data = {\"lunar\": \"\", \"ganzhi\": \"\", \"pzbj\": \"\"}\n",
    "    lunar_div = soup.find('div', class_='lunar')\n",
    "    if lunar_div: lunar_data[\"lunar\"] = lunar_div.text.replace(\"å†œå†\", \"\").strip()\n",
    "    body_div = soup.find('div', class_='body')\n",
    "    if body_div:\n",
    "        ganzhi_p = body_div.find('p')\n",
    "        if ganzhi_p: lunar_data[\"ganzhi\"] = ganzhi_p.text.split('ç”Ÿè‚–å±')[0].strip()     \n",
    "        pzbj_p = ganzhi_p.find_next_sibling('p') if ganzhi_p else None\n",
    "        if pzbj_p and \"å½­ç¥–ç™¾å¿Œ:\" in pzbj_p.text: lunar_data[\"pzbj\"] = pzbj_p.text.replace(\"å½­ç¥–ç™¾å¿Œ:\", \"\").strip()\n",
    "    return lunar_data\n",
    "\n",
    "def get_huangli_data(url, session):\n",
    "    \"\"\"å¤ç”¨sessionçš„å‰¯ç«™æ•°æ®æŠ“å–\"\"\"\n",
    "    try:\n",
    "        response = session.get(url, timeout=8)\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        data = {\"cs\": \"\", \"constellation\": \"\", \"weeks\": \"\", \"zs\": \"\", \"jianxing\": \"\", \"taishen\": \"\", \"num_weeks\":\"\", \"day\": \"\", \"wuxing_day\": \"\",\"jieqi\": \"\",\"next_jieqi\": \"\",\"festival\": \"\",\"next_festival\": \"\",}\n",
    "        shengxiao = \"\"\n",
    "        \n",
    "        for div in soup.find_all('div', class_='hang_left'):\n",
    "            key_element = div.find('p', class_='first_corlor')\n",
    "            value_element = div.find('p', class_='second_color')\n",
    "            if key_element and value_element:\n",
    "                key = key_element.get_text(strip=True)\n",
    "                value = value_element.get_text(strip=True)\n",
    "                if key == 'ç”Ÿè‚–': shengxiao = value\n",
    "                elif key == 'å†²ç…':\n",
    "                    if match := re.search(r'å†²(.+?)ç…(.+)', value): data[\"cs\"] = f\"{shengxiao}æ—¥å†²{match.group(1)} ç…{match.group(2)}\"\n",
    "                elif key == 'æ˜Ÿåº§': data[\"constellation\"] = value\n",
    "                elif key == 'åäºŒå»ºæ˜Ÿ': data[\"jianxing\"] = value\n",
    "                elif key == 'å€¼ç¥': data[\"zs\"] = value\n",
    "                elif key == 'ç¬¬å‡ å‘¨': data[\"num_weeks\"] = value\n",
    "                elif key == 'èƒç¥': data[\"taishen\"] = value.replace('ã€', ' ')\n",
    "                elif key == 'çº³éŸ³': data[\"wuxing_day\"] = value\n",
    "\n",
    "        week_div = soup.find('div', class_='zhong_week')\n",
    "        data[\"weeks\"] = week_div.get_text(strip=True) if week_div else \"\"\n",
    "        qijie_div = soup.find('div', class_='qijie')\n",
    "        if qijie_div: data[\"day\"] = qijie_div.find('a').text.strip() if qijie_div.find('a') else \"\"\n",
    "        \n",
    "        \n",
    "        sucha_div = soup.find('div', class_='sucha')\n",
    "        if sucha_div:\n",
    "            zhoushu_list = sucha_div.find_all('div', class_='zhoushu')\n",
    "            \n",
    "            # è‡ªé€‚åº”å†…å®¹è§£æé€»è¾‘\n",
    "            for idx, zhoushu in enumerate(zhoushu_list):\n",
    "                text = zhoushu.get_text(strip=True)\n",
    "                \n",
    "                # åˆ¤æ–­æ˜¯å¦æ˜¯èŠ‚æ—¥ä¿¡æ¯å—\n",
    "                if any(keyword in text for keyword in [\"èŠ‚æ—¥\", \"èŠ‚æ°”\"]):\n",
    "                    # --------- å¤„ç†èŠ‚æ°”ä¿¡æ¯ ---------\n",
    "                    if \"èŠ‚æ°”\" in text:\n",
    "                        spans = zhoushu.find_all('span')\n",
    "                        if len(spans) >= 3:\n",
    "                            try:\n",
    "                                # æ·»åŠ é¢å¤–æ ¼å¼è¿‡æ»¤(\"å½“å‰èŠ‚æ°”()\"è½¬æ¢ä¸º\"\")\n",
    "                                data[\"jieqi\"] = re.sub(r'å½“å‰èŠ‚æ°”\\(?([^)]*)\\)?', r'\\1', spans[0].text).strip().strip('ï¼ˆï¼‰')\n",
    "                                next_jq = spans[1].text.strip()\n",
    "                                days = spans[2].text.strip()\n",
    "                                data[\"next_jieqi\"] = f\"è·ç¦»ä¸‹ä¸€ä¸ªèŠ‚æ°”{next_jq}è¿˜æœ‰{days}\"\n",
    "                            except IndexError:\n",
    "                                pass\n",
    "                    # --------- å¤„ç†èŠ‚æ—¥ä¿¡æ¯ ---------\n",
    "                    else:\n",
    "                        try:\n",
    "                            # å¢å¼ºå‹èŠ‚æ—¥è§£æ\n",
    "                            if \"æ˜¯\" in text:\n",
    "                                current_part = re.split(r'æ˜¯|\\(', text, 1)[1].split(\"è·ç¦»\")[0]\n",
    "                                festivals = [f.strip(' ã€ï¼ˆï¼‰') for f in re.split(r'[ã€,ï¼Œ]', current_part)]\n",
    "                                data[\"festival\"] = \"ã€\".join(filter(None, festivals))\n",
    "                            else:\n",
    "                                data[\"festival\"] = text.split(\"è·ç¦»\")[0].strip('ï¼ˆ')\n",
    "\n",
    "                            # ä¸‹ä¸ªèŠ‚æ—¥å¤„ç†é€»è¾‘\n",
    "                            next_match = re.search(\n",
    "                                r'è·ç¦»ä¸‹ä¸€ä¸ªèŠ‚æ—¥[ï¼ˆ\\(]*(.*?)[ï¼‰\\)]*è¿˜æœ‰(\\d+)å¤©', \n",
    "                                text\n",
    "                            )\n",
    "                            if next_match:\n",
    "                                data[\"next_festival\"] = f\"è·ç¦»ä¸‹ä¸€ä¸ªèŠ‚æ—¥ï¼ˆ{next_match.group(1).strip()}ï¼‰è¿˜æœ‰{next_match.group(2).strip()}å¤©\"\n",
    "                        except Exception as e:\n",
    "                            print(f\"èŠ‚æ—¥ä¿¡æ¯è§£æé”™è¯¯ï¼š{str(e)}\")\n",
    "\n",
    "        # ç»“æœåå¤„ç†æ¸…æ´—\n",
    "        data[\"festival\"] = re.sub(r'[ï¼ˆ\\(].*?[ï¼‰\\)]', '', data[\"festival\"]).strip('ï¼Œã€')\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"å‰¯ç«™æ•°æ®è·å–å¤±è´¥ {url}: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "def get_shengxiao_info(date_obj, session):\n",
    "    date_str = format_url_date(date_obj)\n",
    "    url = f\"https://m.tthuangli.com/jinrihuangli/xiaoyun_{date_str}.html\"\n",
    "    try:\n",
    "        response = session.get(url, timeout=6)\n",
    "        response.encoding = 'utf-8'\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"ç”Ÿè‚–å‰å‡¶æ¥å£è¯·æ±‚å¤±è´¥: {str(e)}\")\n",
    "        return {\"good_sx\": \"è·å–å¤±è´¥\", \"bad_sx\": \"è·å–å¤±è´¥\"}\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    result = {\"good_sx\": \"\", \"bad_sx\": \"\"}\n",
    "\n",
    "    for container in soup.find_all('div', class_='jibie_tre'):\n",
    "        if container.find('div', class_='teji_desx'):\n",
    "            good_div = container.find('div', class_='sx_info')\n",
    "            if good_div and good_div.span:\n",
    "                result[\"good_sx\"] = good_div.span.get_text().strip()\n",
    "        elif container.find('div', class_='daoshuaisx'):\n",
    "            bad_spans = [\n",
    "                div.span.get_text().strip() \n",
    "                for div in container.find_all('div', class_='shuai_sx_info') \n",
    "                if div.span\n",
    "            ]\n",
    "            result[\"bad_sx\"] = ' '.join(bad_spans)\n",
    "    return result\n",
    "\n",
    "def get_yiji_info(date_str, session):\n",
    "    url = f\"https://m.tthuangli.com/jinrihuangli/yiji_{date_str}.html\"\n",
    "\n",
    "    try:\n",
    "        # å‘é€HTTPSè¯·æ±‚\n",
    "        response = session.get(url, timeout=10) \n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # ç¼–ç å¤„ç†\n",
    "        if response.encoding == 'ISO-8859-1':\n",
    "            response.encoding = response.apparent_encoding\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        def extract_by_css():\n",
    "            container = soup.select_one('div.three_hang table')\n",
    "            ji_numbers = container.select_one('td:nth-of-type(1) .second_color_ji span').text.strip()\n",
    "            auspicious_time = container.select_one('td:nth-of-type(2) .second_color_ji span').text.strip()\n",
    "            return ji_numbers, auspicious_time\n",
    "\n",
    "        def extract_conflict_info(soup):\n",
    "            \"\"\" è§£æç”Ÿè‚–å†²å®³ä¿¡æ¯ \"\"\"\n",
    "            # ç²¾ç¡®CSSè·¯å¾„å®šä½\n",
    "            conflict_div = soup.select_one('div.hljiexi div.sucha div.chong_sx')\n",
    "            if not conflict_div:\n",
    "                raise ValueError(\"æœªæ‰¾åˆ°ç”Ÿè‚–å†²çªä¿¡æ¯\")\n",
    "                \n",
    "            # è·å–åŸå§‹æ–‡æœ¬å¹¶å¤„ç†\n",
    "            conflict_text = conflict_div.text.strip()\n",
    "            \n",
    "            # å¤šé‡æ¸…æ´—ä¿éšœæ ¼å¼\n",
    "            conflict_text = conflict_text.replace('\\u3000', ' ')  # æ›¿æ¢å…¨è§’ç©ºæ ¼\n",
    "            conflict_text = conflict_text.replace('  ', ' ')     # åˆå¹¶è¿ç»­ç©ºæ ¼\n",
    "            return conflict_text\n",
    "\n",
    "        # æ•°æ®æå–\n",
    "        try:\n",
    "            lucky_numbers, hour_range = extract_by_css()\n",
    "            conflict_text = extract_conflict_info(soup)\n",
    "        except AttributeError as e:\n",
    "            print(f\"å…ƒç´ å®šä½å¤±è´¥ï¼š{str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # æ•°æ®æ ¼å¼åŒ–\n",
    "        formatted_data = {\n",
    "            \"lucky_num\": lucky_numbers.replace(' ', '').replace(',', 'ã€'),\n",
    "            \"noble_time\": hour_range.replace('ã€', '-').replace('ç‚¹', '') + 'ç‚¹',\n",
    "            \"conflict_sx\": conflict_text\n",
    "        }\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"ç½‘ç»œè¯·æ±‚å¤±è´¥ï¼š{str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ç¨‹åºå¼‚å¸¸ï¼š{str(e)}\")\n",
    "\n",
    "    return formatted_data\n",
    "\n",
    "def get_color_info(date_str, session):\n",
    "    url = f\"https://m.tthuangli.com/jinrihuangli/wuxingchuanyi_{date_str}.html\" \n",
    "    try:\n",
    "        response = session.get(url, timeout=10)  # å¤ç”¨session\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        result = {\n",
    "            'good_color': '',\n",
    "            'bad_color': ''\n",
    "        }\n",
    "\n",
    "        # å¤„ç†å¤§å‰è‰²\n",
    "        djse = soup.find('div', class_='djse')\n",
    "        if djse:\n",
    "            dj_colors = djse.get_text(strip=True).split('ï¼š')[1]\n",
    "            result['good_color'] = dj_colors.replace('ã€', ' ')  # ç›´æ¥æ›¿æ¢é¡¿å·ä¸ºç©ºæ ¼\n",
    "            \n",
    "        # å¤„ç†ä¸å®œè‰²\n",
    "        byse = soup.find('div', class_='byse')\n",
    "        if byse:\n",
    "            by_colors = byse.get_text(strip=True).split('ï¼š')[1]\n",
    "            result['bad_color'] = by_colors.replace('ã€', ' ')  # ç›´æ¥æ›¿æ¢é¡¿å·ä¸ºç©ºæ ¼\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[å‡ºç°å¼‚å¸¸] {str(e)}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_lucky_time(date_str, session):\n",
    "    url = f'https://m.tthuangli.com/jinrihuangli/jishi_{date_str}.html'\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        lucky_time_list = []\n",
    "        \n",
    "        for block in soup.find_all('div', class_='jiri_ji'):\n",
    "            title_tag = block.find('div', class_='jiri_ji_tit')\n",
    "            if not title_tag:\n",
    "                continue\n",
    "                \n",
    "            title_parts = title_tag.text.strip().split()\n",
    "            time_name = title_parts[-1] if title_parts else ''\n",
    "            \n",
    "            time_range_tag = block.find('div', class_='juti_time')\n",
    "            time_range = time_range_tag.text.strip() if time_range_tag else ''\n",
    "            \n",
    "            if time_name and time_range:\n",
    "                lucky_time_list.append(f\"{time_name} {time_range}\")\n",
    "\n",
    "        # ä¿®æ”¹å…³é”®ç‚¹ï¼šåˆ é™¤æœ«å°¾çš„ + \", \"\n",
    "        return {\n",
    "            \"lucky_time\": \", \".join(lucky_time_list) if lucky_time_list else \"\"\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)}, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# ====================== ä¸»æŠ“å–é€»è¾‘ ======================\n",
    "# ...ï¼ˆä¿æŒä¹‹å‰çš„å·¥å…·å‡½æ•°å’ŒæŠ“å–å‡½æ•°ä¸å˜ï¼Œä»…ä¿®æ”¹ä¸»å¤„ç†ç»“æ„ï¼‰...\n",
    "\n",
    "def scrape_single_date(date_str, session):\n",
    "    \"\"\"å•ä¸ªæ—¥æœŸçš„å®Œæ•´æŠ“å–æµç¨‹\"\"\"\n",
    "    main_url = f\"https://www.huangli.net.cn/{date_str}.html\"\n",
    "    \n",
    "    try:\n",
    "        # ===== ä¸»ç«™æ•°æ®æŠ“å– =====\n",
    "        response = session.get(main_url, timeout=8)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # è§£æåŸºç¡€æ•°æ®\n",
    "        base_data = {\n",
    "            \"datekey\": parse_date_from_url(main_url),\n",
    "            **extract_lunar_info(soup),\n",
    "            **extract_suitable_items(soup),\n",
    "            **parse_god_positions(soup),  # å•ç‹¬è§£æè´¢ç¥ä½\n",
    "            **parse_jixiong_items(soup)    # è§£æå‰ç¥å®œè¶‹å’Œå‡¶ç…å®œå¿Œ\n",
    "        }\n",
    "\n",
    "        # ===== å¹¶å‘è·å–å‰¯ç«™æ•°æ® =====\n",
    "        date_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = {\n",
    "                \"sub\": executor.submit(get_huangli_data, f\"https://m.tthuangli.com/jinrihuangli/{date_str}.html\", session),\n",
    "                \"sx\": executor.submit(get_shengxiao_info, date_obj, session),\n",
    "                \"color\": executor.submit(get_color_info, date_str, session),\n",
    "                \"yiji\": executor.submit(get_yiji_info, date_str, session),\n",
    "                \"lucky_time\": executor.submit(get_lucky_time, date_str, session)\n",
    "            }\n",
    "\n",
    "            # åˆå¹¶å‰¯ç«™æ•°æ®ï¼ˆè‡ªåŠ¨å±•å¼€åµŒå¥—å­—æ®µï¼‰\n",
    "            for key, future in futures.items():\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    # ç‰¹æ®Šå¤„ç†å„ä¸ªå‰¯ç«™çš„æ•°æ®ç»“æ„\n",
    "                    if key == \"sub\":\n",
    "                        base_data.update({\n",
    "                            \"cs\": result.get(\"cs\", \"\"),\n",
    "                            \"constellation\": result.get(\"constellation\", \"\"),\n",
    "                            \"weeks\": result.get(\"weeks\", \"\"),\n",
    "                            \"zs\": result.get(\"zs\", \"\"),\n",
    "                            \"jianxing\": result.get(\"jianxing\", \"\"),\n",
    "                            \"taishen\": result.get(\"taishen\", \"\"),\n",
    "                            \"num_weeks\": result.get(\"num_weeks\", \"\"),\n",
    "                            \"day\": result.get(\"day\", \"\"),\n",
    "                            \"wuxing_day\": result.get(\"wuxing_day\", \"\"),\n",
    "                            \"jieqi\": result.get(\"jieqi\", \"\"),\n",
    "                            \"next_jieqi\": result.get(\"next_jieqi\", \"\"),\n",
    "                            \"festival\": result.get(\"festival\", \"\"),\n",
    "                            \"next_festival\": result.get(\"next_festival\", \"\")\n",
    "                        })\n",
    "                    elif key == \"sx\":\n",
    "                        base_data[\"good_sx\"] = result.get(\"good_sx\", \"\")\n",
    "                        base_data[\"bad_sx\"] = result.get(\"bad_sx\", \"\")\n",
    "                    elif key == \"color\":\n",
    "                        base_data[\"good_color\"] = result.get(\"good_color\", \"\")\n",
    "                        base_data[\"bad_color\"] = result.get(\"bad_color\", \"\")\n",
    "                    elif key == \"yiji\":\n",
    "                        base_data.update({\n",
    "                            \"lucky_num\": result.get(\"lucky_num\", \"\"),\n",
    "                            \"noble_time\": result.get(\"noble_time\", \"\"),\n",
    "                            \"conflict_sx\": result.get(\"conflict_sx\", \"\")\n",
    "                        })\n",
    "                    elif key == \"lucky_time\":\n",
    "                        base_data.update({\n",
    "                            \"lucky_time\": result.get(\"lucky_time\", \"\"),\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"å‰¯ç«™ {key} æ•°æ®è·å–å¤±è´¥: {str(e)}\")\n",
    "\n",
    "        return base_data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"ä¸»æµç¨‹å¼‚å¸¸ {date_str}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# å°è£…å…¬å…±æŸ¥æ‰¾å‡½æ•°\n",
    "def find_item_by_title(title,soup):\n",
    "    for div in soup.find_all('div', class_='item'):\n",
    "        h4 = div.find('h4')\n",
    "        if h4 and h4.text.strip() == title:\n",
    "            return div\n",
    "    return None\n",
    "\n",
    "# ========= æ–°å¢çš„è§£æå‡½æ•° =========\n",
    "def parse_god_positions(soup):\n",
    "    \"\"\"è§£æè´¢ç¥ä½ä¿¡æ¯\"\"\"\n",
    "    god_data = {\"godposition\": \"\"}\n",
    "\n",
    "    try:\n",
    "        caishen_div = find_item_by_title('è´¢ç¥ä½',soup)\n",
    "        if not caishen_div:\n",
    "            raise ValueError(\"æœªæ‰¾åˆ°è´¢ç¥ä½åŒºå—\")\n",
    "\n",
    "        caishen_data = {}\n",
    "        for li in caishen_div.find('ul').find_all('li'):\n",
    "            key, val = li.text.strip().split('ï¼š', 1)\n",
    "            caishen_data[key] = val\n",
    "        \n",
    "        god_data[\"godposition\"] = f\"å–œç¥åœ¨{caishen_data['å–œç¥']} è´¢ç¥åœ¨{caishen_data['è´¢ç¥']} ç¦ç¥åœ¨{caishen_data['ç¦ç¥']}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"è´¢ç¥ä½è§£æå¤±è´¥: {str(e)}\")\n",
    "    return god_data\n",
    "\n",
    "def parse_jixiong_items(soup):\n",
    "    \"\"\"è§£æå‰ç¥å®œè¶‹å’Œå‡¶ç…å®œå¿Œ\"\"\"\n",
    "    jixiong_data = {\"jsyq\": \"\", \"xsyq\": \"\"}\n",
    "    try:\n",
    "        # å‰ç¥å®œè¶‹\n",
    "        jsyq_div = soup.find('h4', string='å‰ç¥å®œè¶‹').find_next('ul', class_='list-2')\n",
    "        jixiong_data[\"jsyq\"] = ' '.join([li.text.strip() for li in jsyq_div.find_all('li')])\n",
    "        \n",
    "        # å‡¶ç…å®œå¿Œ\n",
    "        xsyq_div = soup.find('h4', string='å‡¶ç…å®œå¿Œ').find_next('ul', class_='list-2')\n",
    "        jixiong_data[\"xsyq\"] = ' '.join([li.text.strip() for li in xsyq_div.find_all('li')])\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"å‰å‡¶ä¿¡æ¯è§£æå¤±è´¥: {str(e)}\")\n",
    "    return jixiong_data\n",
    "\n",
    "# ====================== å­˜å‚¨æ¨¡å— ======================\n",
    "class HlDataSaver:\n",
    "    def __init__(self, filename=''):\n",
    "        self.filename = filename\n",
    "\n",
    "    def _datekey_to_datetime(self, item):\n",
    "        \"\"\"ç»Ÿä¸€çš„æ—¥æœŸè½¬æ¢æ–¹æ³•ï¼ˆåŒ¹é…ä½ çš„å…·ä½“æ—¥æœŸæ ¼å¼ï¼‰\"\"\"\n",
    "        return datetime.strptime(item[\"datekey\"], \"%Y/%m/%d %H:%M:%S\")\n",
    "\n",
    "    def _sorted_data(self, data_list):\n",
    "        \"\"\"å°è£…ç»Ÿä¸€æ’åºé€»è¾‘\"\"\"\n",
    "        return sorted(\n",
    "            data_list,\n",
    "            key=self._datekey_to_datetime,\n",
    "            reverse=False  # é¡ºåºï¼šæœ€æ—§->æœ€æ–°\n",
    "        )\n",
    "    \n",
    "    def _load_existing(self):\n",
    "        \"\"\"åŠ è½½ç°æœ‰æ•°æ®å¹¶ä¿æŒå†…å­˜æ•°æ®æœ‰æ•ˆæ€§\"\"\"\n",
    "        if not os.path.exists(self.filename):\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            with open(self.filename, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                # éªŒè¯æ•°æ®æ ¼å¼çš„æœ‰æ•ˆæ€§\n",
    "                return data.get(\"data\", []) if isinstance(data, dict) else []\n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"åŠ è½½å†å²æ•°æ®å¤±è´¥ï¼Œé‡ç½®å­˜å‚¨: {str(e)}\")\n",
    "            return []\n",
    "        \n",
    "    def save_incrementally(self, new_data):\n",
    "        \"\"\"å¢é‡å­˜å‚¨å¹¶ä¿æŒæŒ‰æ—¥æœŸå‡åºæ’åˆ—ï¼ˆçº¿ç¨‹å®‰å…¨ç‰ˆï¼‰\"\"\"\n",
    "        existing = self._load_existing()\n",
    "        existing_dates = {item[\"datekey\"]: idx for idx, item in enumerate(existing)}\n",
    "        modified = False\n",
    "\n",
    "        # å¤„ç†æ›´æ–°å’Œæ–°å¢\n",
    "        for item in new_data:\n",
    "            datekey = item[\"datekey\"]\n",
    "            \n",
    "            # å­˜åœ¨æ€§æ£€æŸ¥\n",
    "            if datekey in existing_dates:\n",
    "                idx = existing_dates[datekey]\n",
    "                \n",
    "                # å“ˆå¸Œæ¯”å¯¹é¿å…æ— æ„ä¹‰æ›´æ–°\n",
    "                if hash(frozenset(existing[idx].items())) != hash(frozenset(item.items())):\n",
    "                    existing[idx] = item\n",
    "                    modified = True\n",
    "            else:\n",
    "                # æŒ‰é¡ºåºæ’å…¥è€Œä¸æ˜¯ç›´æ¥append\n",
    "                insert_pos = next(\n",
    "                    (i for i, x in enumerate(existing) \n",
    "                     if self._datekey_to_datetime(x) > self._datekey_to_datetime(item)),\n",
    "                    len(existing)\n",
    "                )\n",
    "                \n",
    "                existing.insert(insert_pos, item)\n",
    "                existing_dates[datekey] = insert_pos\n",
    "                modified = True\n",
    "\n",
    "        # åŒé‡ç¡®è®¤æ’åºé€»è¾‘\n",
    "        if modified:\n",
    "            # æœ€ç»ˆä¿å­˜å‰çš„å…¨é¢æ’åºï¼ˆé˜²æ­¢ä¸­é—´æ’å…¥ä½ç½®è®¡ç®—é”™è¯¯ï¼‰\n",
    "            sorted_data = self._sorted_data(existing)\n",
    "            \n",
    "            # ç”Ÿæˆç»Ÿä¸€æ ¼å¼çš„ä¿å­˜æ•°æ®\n",
    "            save_data = {\n",
    "                \"version\": 2.0,\n",
    "                \"generated_at\": datetime.now().isoformat(),\n",
    "                \"data\": sorted_data\n",
    "            }\n",
    "\n",
    "            # åŸå­åŒ–ä¿å­˜ï¼ˆé¿å…å†™æ–‡ä»¶ä¸­é€”å‡ºé”™ç ´åæ•°æ®ï¼‰\n",
    "            temp_file = f\"{self.filename}.tmp\"\n",
    "            with open(temp_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(save_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            # æ›¿æ¢åŸæ–‡ä»¶ï¼ˆè·¨å¹³å°å®‰å…¨æ“ä½œï¼‰\n",
    "            if os.path.exists(self.filename):\n",
    "                os.replace(temp_file, self.filename)\n",
    "            else:\n",
    "                os.rename(temp_file, self.filename)\n",
    "\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# ====================== å¢å¼ºç‰ˆæ•°æ®åŠ è½½å™¨ ======================\n",
    "class HlDataLoader:\n",
    "    def __init__(self, filename=''):\n",
    "        self.filename = filename\n",
    "        self._data = []\n",
    "        self._index = {}  # datekeyåˆ°ç´¢å¼•çš„æ˜ å°„\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®\"\"\"\n",
    "        if not os.path.exists(self.filename):\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            with open(self.filename, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            # å…¼å®¹æ–°æ—§æ ¼å¼å¤„ç†\n",
    "            raw_data = data.get('data', [])\n",
    "            if isinstance(raw_data, dict):  # è½¬æ¢æ—§æ ¼å¼çš„å­—å…¸æ•°æ®\n",
    "                self._data = list(raw_data.values())\n",
    "            else:\n",
    "                self._data = raw_data\n",
    "                \n",
    "            # æ„å»ºç´¢å¼•\n",
    "            self._index = {item['datekey']: idx for idx, item in enumerate(self._data)}\n",
    "        except Exception as e:\n",
    "            logging.error(f\"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}\")\n",
    "\n",
    "    def get_by_date(self, date_str):\n",
    "        \"\"\"æŒ‰æ—¥æœŸæŸ¥è¯¢æ•°æ®ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰\"\"\"\n",
    "        target_key = self._normalize_datekey(date_str)\n",
    "        return self._data[self._index.get(target_key, -1)]\n",
    "\n",
    "    def filter_data(self, start_date=None, end_date=None, keywords=None):\n",
    "        \"\"\"é«˜çº§æ•°æ®è¿‡æ»¤\"\"\"\n",
    "        filtered = []\n",
    "        for item in self._data:\n",
    "            # æ—¥æœŸèŒƒå›´è¿‡æ»¤\n",
    "            date_obj = datetime.strptime(item['datekey'], \"%Y/%m/%d %H:%M:%S\")\n",
    "            if start_date and date_obj < start_date:\n",
    "                continue\n",
    "            if end_date and date_obj > end_date:\n",
    "                continue\n",
    "            \n",
    "            # å…³é”®è¯æœç´¢\n",
    "            if keywords:\n",
    "                search_area = ' '.join(str(v) for v in item.values())\n",
    "                if not any(kw.lower() in search_area.lower() for kw in keywords):\n",
    "                    continue\n",
    "                    \n",
    "            filtered.append(item)\n",
    "        \n",
    "        # æ’åºä¿éšœ\n",
    "        filtered.sort(key=lambda x: x['datekey'])\n",
    "        return filtered\n",
    "\n",
    "    def _normalize_datekey(self, date_input):\n",
    "        \"\"\"æ—¥æœŸæ ¼å¼ç»Ÿä¸€å¤„ç†\"\"\"\n",
    "        if isinstance(date_input, datetime):\n",
    "            return date_input.strftime(\"%Y/%m/%d 00:00:00\")\n",
    "            \n",
    "        try:\n",
    "            # æ”¯æŒå¤šç§æ—¥æœŸå­—ç¬¦ä¸²æ ¼å¼\n",
    "            formats = [\n",
    "                \"%Y/%m/%d %H:%M:%S\",\n",
    "                \"%Y-%m-%d\",\n",
    "                \"%Y%m%d\"\n",
    "            ]\n",
    "            for fmt in formats:\n",
    "                try:\n",
    "                    dt = datetime.strptime(date_input, fmt)\n",
    "                    return dt.strftime(\"%Y/%m/%d 00:00:00\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            raise ValueError(\"æ— æ³•è¯†åˆ«çš„æ—¥æœŸæ ¼å¼\")\n",
    "        except:\n",
    "            raise ValueError(\"æ— æ•ˆçš„æ—¥æœŸè¾“å…¥\")\n",
    "        \n",
    "    # ç»Ÿè®¡åˆ†æåŠŸèƒ½ï¼ˆç¤ºä¾‹ï¼‰\n",
    "    def get_statistics(self):\n",
    "        \"\"\"è·å–é»„å†æ•°æ®ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        stats = {\n",
    "            'total_days': len(self._data),\n",
    "            'most_common_jsyq': Counter(item['jsyq'] for item in self._data).most_common(5),\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "    # æ•°æ®å®Œæ•´æ€§æ£€æµ‹ï¼ˆç¤ºä¾‹ï¼‰\n",
    "    def validate_data(self):\n",
    "        \"\"\"æ ¡éªŒæ•°æ®å®Œæ•´æ€§\"\"\"\n",
    "        missing_fields = []\n",
    "        for idx, item in enumerate(self._data):\n",
    "            if 'datekey' not in item:\n",
    "                missing_fields.append(f\"ç´¢å¼• {idx} ç¼ºå°‘datekeyå­—æ®µ\")\n",
    "                continue\n",
    "                \n",
    "            # æ ¡éªŒæ—¥æœŸæ ¼å¼åˆæ³•æ€§\n",
    "            try:\n",
    "                datetime.strptime(item['datekey'], \"%Y/%m/%d %H:%M:%S\")\n",
    "            except:\n",
    "                missing_fields.append(f\"ç´¢å¼• {idx} datekeyæ ¼å¼é”™è¯¯: {item['datekey']}\")\n",
    "                \n",
    "        return missing_fields\n",
    "    \n",
    "    def save_data(self, output_file=None):\n",
    "        \"\"\"ä¿å­˜æ•°æ®å¹¶è‡ªåŠ¨æ’åºï¼Œè¦†ç›–é‡å¤æ—¥æœŸ\"\"\"\n",
    "        try:\n",
    "            target_file = output_file or self.filename\n",
    "            \n",
    "            # è½¬æ¢å­—å…¸å»é‡å¹¶ä¿ç•™æœ€åå‡ºç°çš„æ—¥æœŸæ•°æ®\n",
    "            data_dict = {}\n",
    "            duplicate_count = 0\n",
    "            for item in self._data:\n",
    "                datekey = item.get('datekey')\n",
    "                if datekey:\n",
    "                    if datekey in data_dict:\n",
    "                        duplicate_count += 1\n",
    "                    data_dict[datekey] = item\n",
    "\n",
    "            # æ’åºå¤„ç†\n",
    "            sorted_data = sorted(data_dict.values(), key=self._datekey_to_datetime)\n",
    "            \n",
    "            # æ„å»ºä¿å­˜æ•°æ®ç»“æ„\n",
    "            save_data = {\n",
    "                \"version\": 1.2,\n",
    "                \"generated_at\": datetime.now().isoformat(),\n",
    "                \"data\": sorted_data\n",
    "            }\n",
    "\n",
    "            # å†™å…¥æ–‡ä»¶\n",
    "            with open(target_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(save_data, f, indent=2, ensure_ascii=False)\n",
    "                \n",
    "        except PermissionError:\n",
    "            print(\"é”™è¯¯ï¼šæ²¡æœ‰æ–‡ä»¶å†™å…¥æƒé™\")\n",
    "        except Exception as e:\n",
    "            print(f\"ä¿å­˜å¤±è´¥: {str(e)}\")\n",
    "\n",
    "    def _datekey_to_datetime(self, item):\n",
    "        \"\"\"å†…éƒ¨æ—¥æœŸè½¬æ¢æ–¹æ³•\"\"\"\n",
    "        return datetime.strptime(item['datekey'], \"%Y/%m/%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "# ====================== æ‰§è¡Œå…¥å£ ======================\n",
    "def main(start_date, end_date):\n",
    "    \"\"\"ä¸»æ§åˆ¶å‡½æ•°\"\"\"\n",
    "    # ç”Ÿæˆæ—¥æœŸåˆ—è¡¨\n",
    "    date_objs = [start_date + timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n",
    "    target_dates = [format_url_date(d) for d in date_objs]\n",
    "\n",
    "    # åˆ›å»ºå…±äº«ä¼šè¯\n",
    "    session = create_session()\n",
    "\n",
    "    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†æ—¥æœŸ\n",
    "    # å­˜å‚¨ä¼˜åŒ–ï¼šé€æ¡å¤„ç†+å®æ—¶ä¿å­˜\n",
    "    saver = HlDataSaver(filename='huangli_data.json')\n",
    "    loader = HlDataLoader(filename='huangli_data.json')\n",
    "\n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        futures = {executor.submit(scrape_single_date, date_str, session): date_str \n",
    "                  for date_str in target_dates}\n",
    "        \n",
    "        # å®æ—¶å¤„ç†å®Œæˆç»“æœ\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            date_str = futures[future]\n",
    "            try:\n",
    "                if data := future.result():\n",
    "                    # å®æ—¶ä¿å­˜å•æ—¥æ•°æ®\n",
    "                    saver.save_incrementally([data])\n",
    "                    # loader._data.append(data)\n",
    "                    # loader.save_data()\n",
    "\n",
    "                    results.append(data)\n",
    "                    logging.info(f\"âˆš {date_str} å¤„ç†å¹¶ä¿å­˜å®Œæˆ\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Ã— {date_str} å¤„ç†å¤±è´¥: {str(e)}\")\n",
    "\n",
    "    return {\n",
    "        \"status\": 0,\n",
    "        \"saved_days\": len(results),\n",
    "        \"failed_days\": len(target_dates)-len(results)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 17:32:44,217 [INFO] âˆš 2025-3-1 å¤„ç†å¹¶ä¿å­˜å®Œæˆ\n",
      "2025-02-28 17:32:44,221 [INFO] âˆš 2025-3-2 å¤„ç†å¹¶ä¿å­˜å®Œæˆ\n",
      "2025-02-28 17:32:44,223 [INFO] âˆš 2025-3-3 å¤„ç†å¹¶ä¿å­˜å®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ“ä½œå®Œæˆ | æˆåŠŸ: 3 å¤© | å¤±è´¥: 0 å¤©\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # é…ç½®æŠ“å–æ—¥æœŸèŒƒå›´\n",
    "    start_date = datetime(2025, 3, 1)\n",
    "    end_date = datetime(2025, 3, 3)\n",
    "    \n",
    "    # æ‰§è¡ŒæŠ“å–\n",
    "    output = main(start_date, end_date)\n",
    "    \n",
    "    # è¾“å‡ºç»“æœ\n",
    "    print(f\"æ“ä½œå®Œæˆ | æˆåŠŸ: {output['saved_days']} å¤© | å¤±è´¥: {output['failed_days']} å¤©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== ä½¿ç”¨ç¤ºä¾‹ ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # åˆå§‹åŒ–åŠ è½½å™¨\n",
    "    loader = HlDataLoader(filename='huangli_data.json')\n",
    "    \n",
    "    # æŸ¥è¯¢å•ä¸ªæ—¥æœŸï¼ˆå¤šç§æ ¼å¼æ”¯æŒï¼‰\n",
    "    print(loader.get_by_date(\"2025-03-02\"))  # æ ‡å‡†æ—¥æœŸæ ¼å¼    \n",
    "    \n",
    "    # å¤æ‚æŸ¥è¯¢ï¼š2025å¹´3æœˆåŒ…å«\"æ˜ŸæœŸæ—¥\"çš„æ—¥å­\n",
    "    results = loader.filter_data(\n",
    "        start_date=datetime(2025,3,1),\n",
    "        end_date=datetime(2025,3,31),\n",
    "        keywords=[\"æ˜ŸæœŸæ—¥\"]\n",
    "    )\n",
    "    \n",
    "    # æ˜¾ç¤ºæŸ¥è¯¢ç»“æœ\n",
    "    for item in results:\n",
    "        print(f\"{item['datekey']}: {item.get('lucky_num')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = loader.get_statistics()\n",
    "\n",
    "# è¾“å‡ºç»Ÿè®¡ç»“æœ\n",
    "print(f\"æ€»å¤©æ•°: {stats['total_days']}\")\n",
    "print(\"\\nå¸¸è§å‰ç¥è¶‹æ—¥Top5:\")\n",
    "for item, count in stats['most_common_jsyq']:\n",
    "    print(f\"{item}: {count} æ¬¡\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = loader.validate_data()\n",
    "\n",
    "# æ ¹æ®ç»“æœå¤„ç†\n",
    "if not errors:\n",
    "    print(\"ğŸŒŸ æ•°æ®æ•ˆéªŒé€šè¿‡ï¼Œæ— å¼‚å¸¸\")\n",
    "else:\n",
    "    print(f\"å‘ç° {len(errors)} é¡¹é—®é¢˜:\")\n",
    "    for error in errors:\n",
    "        print(f\"âŒ {error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = HlDataLoader(filename='huangli_data.json')\n",
    "\n",
    "# è·å–ç»Ÿè®¡ä¿¡æ¯\n",
    "stats = loader.get_statistics()\n",
    "\n",
    "# éªŒè¯æ•°æ®\n",
    "errors = loader.validate_data()\n",
    "\n",
    "# ç”ŸæˆæŠ¥å‘Š\n",
    "report = {\n",
    "    \"data_source\": loader.filename,\n",
    "    \"days_covered\": f\"{stats['total_days']} å¤©\",\n",
    "    \"data_quality\": \"PASS\" if not errors else f\"{len(errors)} errors\",\n",
    "    \"recommendations\": [\n",
    "        \"å®šæœŸå¤‡ä»½æ•°æ®\",\n",
    "        \"å¼‚å¸¸æ—¥æœŸéœ€è¦äººå·¥å¤æ ¸\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(json.dumps(report, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸä¿å­˜145æ¡æ•°æ®\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = \"https://jinpaper.com/pages/copy-of-2022-%E6%AF%8F%E6%9C%88%E8%AF%9E%E8%BE%B0\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "data = []\n",
    "current_month = None\n",
    "remain_rows = 0  # rowspanå‰©ä½™è¡Œæ•°\n",
    "\n",
    "for tr in soup.select('table[dir=\"ltr\"] tr')[2:]:  # è·³è¿‡å‰ä¸¤è¡Œè¡¨å¤´\n",
    "    tds = tr.find_all('td')\n",
    "    \n",
    "    # æ£€æµ‹æœˆä»½è¡Œï¼šå¸¦rowspanå±æ€§çš„å•å…ƒæ ¼\n",
    "    month_td = next((td for td in tds if td.get('rowspan') and 'æœˆ' in td.text), None)\n",
    "    \n",
    "    if month_td:\n",
    "        # å¤„ç†æ–°æœˆä»½è¡Œ\n",
    "        current_month = month_td.get_text(strip=True).replace(\"ï¼‰\", \"\").replace(\"ï¼ˆ\", \"-\")\n",
    "        remain_rows = int(month_td['rowspan']) - 1  # ç®—ä¸Šå½“å‰è¡Œ\n",
    "        \n",
    "        # æå–å½“å‰è¡Œæ•°æ®\n",
    "        other_tds = [td for td in tds if td != month_td]\n",
    "        if len(other_tds) >= 3:\n",
    "            solar = other_tds[0].get_text(strip=True).replace(\"å·\", \"æ—¥\")\n",
    "            lunar = other_tds[1].get_text(strip=True)\n",
    "            festival = other_tds[2].get_text(strip=True)\n",
    "            \n",
    "            data.append({\n",
    "                \"date\": solar,\n",
    "                \"nongli\": lunar,\n",
    "                \"festival\": festival\n",
    "            })\n",
    "    else:\n",
    "        # å¤„ç†æ™®é€šæ•°æ®è¡Œ\n",
    "        if remain_rows > 0 and len(tds) >= 3:\n",
    "            solar = tds[0].get_text(strip=True).replace(\"å·\", \"æ—¥\")\n",
    "            lunar = tds[1].get_text(strip=True)\n",
    "            festival = tds[2].get_text(strip=True)\n",
    "            \n",
    "            data.append({\n",
    "                \"date\": solar,\n",
    "                \"nongli\": lunar,\n",
    "                \"festival\": festival\n",
    "            })\n",
    "            remain_rows -= 1\n",
    "\n",
    "# ä¿å­˜ä¸ºJSON\n",
    "with open('2022_correct_festivals.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2, separators=(',', ': '))\n",
    "\n",
    "print(f\"æˆåŠŸä¿å­˜{len(data)}æ¡æ•°æ®\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
