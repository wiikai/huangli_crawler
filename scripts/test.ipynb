{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import concurrent.futures\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import threading\n",
    "from collections import Counter\n",
    "\n",
    "# ====================== 配置部分 ======================\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s [%(levelname)s] %(message)s')\n",
    "\n",
    "# ====================== 网络连接配置 ======================\n",
    "def create_session(retries=3):\n",
    "    \"\"\"创建带重试机制的会话对象\"\"\"\n",
    "    session = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        backoff_factor=0.3,\n",
    "        status_forcelist=(500, 502, 504)\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry,pool_connections=50,pool_maxsize=100)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "# ====================== 工具函数 ======================\n",
    "def format_url_date(date_obj):\n",
    "    \"\"\"生成无前导零的日期字符串（跨平台兼容）\"\"\"\n",
    "    return f\"{date_obj.year}-{date_obj.month}-{date_obj.day}\"\n",
    "\n",
    "def parse_date_from_url(url):\n",
    "    \"\"\"从URL解析日期\"\"\"\n",
    "    path = urlparse(url).path\n",
    "    date_part = path.split('/')[-1].replace('.html', '')\n",
    "    return datetime.strptime(date_part, \"%Y-%m-%d\").strftime(\"%Y/%m/%d 00:00:00\")\n",
    "\n",
    "# ====================== 数据抓取函数 ======================\n",
    "def extract_suitable_items(soup):\n",
    "    \"\"\"提取宜/忌信息\"\"\"\n",
    "    result = {\"yi\": \"\", \"ji\": \"\"}\n",
    "    for suitable_div in soup.find_all('div', class_='suitable'):\n",
    "        span = suitable_div.find('span')\n",
    "        if not span: continue\n",
    "        category = span.text.strip()\n",
    "        if category not in [\"宜\", \"忌\"]: continue\n",
    "        content_div = suitable_div.find_next_sibling('div', class_='suitable_con')\n",
    "        if not content_div: continue\n",
    "        items = [li.span.text.strip() for li in content_div.find_all('li')]\n",
    "        result[\"yi\" if category == \"宜\" else \"ji\"] = ' '.join(items)\n",
    "    return result\n",
    "\n",
    "def extract_lunar_info(soup):\n",
    "    \"\"\"提取农历/干支/生肖/彭祖百忌\"\"\"\n",
    "    lunar_data = {\"lunar\": \"\", \"ganzhi\": \"\", \"pzbj\": \"\"}\n",
    "    lunar_div = soup.find('div', class_='lunar')\n",
    "    if lunar_div: lunar_data[\"lunar\"] = lunar_div.text.replace(\"农历\", \"\").strip()\n",
    "    body_div = soup.find('div', class_='body')\n",
    "    if body_div:\n",
    "        ganzhi_p = body_div.find('p')\n",
    "        if ganzhi_p: lunar_data[\"ganzhi\"] = ganzhi_p.text.split('生肖属')[0].strip()     \n",
    "        pzbj_p = ganzhi_p.find_next_sibling('p') if ganzhi_p else None\n",
    "        if pzbj_p and \"彭祖百忌:\" in pzbj_p.text: lunar_data[\"pzbj\"] = pzbj_p.text.replace(\"彭祖百忌:\", \"\").strip()\n",
    "    return lunar_data\n",
    "\n",
    "def get_huangli_data(url, session):\n",
    "    \"\"\"复用session的副站数据抓取\"\"\"\n",
    "    try:\n",
    "        response = session.get(url, timeout=8)\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        data = {\"cs\": \"\", \"constellation\": \"\", \"weeks\": \"\", \"zs\": \"\", \"jianxing\": \"\", \"taishen\": \"\", \"num_weeks\":\"\", \"day\": \"\", \"wuxing_day\": \"\",\"jieqi\": \"\",\"next_jieqi\": \"\",\"festival\": \"\",\"next_festival\": \"\",}\n",
    "        shengxiao = \"\"\n",
    "        \n",
    "        for div in soup.find_all('div', class_='hang_left'):\n",
    "            key_element = div.find('p', class_='first_corlor')\n",
    "            value_element = div.find('p', class_='second_color')\n",
    "            if key_element and value_element:\n",
    "                key = key_element.get_text(strip=True)\n",
    "                value = value_element.get_text(strip=True)\n",
    "                if key == '生肖': shengxiao = value\n",
    "                elif key == '冲煞':\n",
    "                    if match := re.search(r'冲(.+?)煞(.+)', value): data[\"cs\"] = f\"{shengxiao}日冲{match.group(1)} 煞{match.group(2)}\"\n",
    "                elif key == '星座': data[\"constellation\"] = value\n",
    "                elif key == '十二建星': data[\"jianxing\"] = value\n",
    "                elif key == '值神': data[\"zs\"] = value\n",
    "                elif key == '第几周': data[\"num_weeks\"] = value\n",
    "                elif key == '胎神': data[\"taishen\"] = value.replace('、', ' ')\n",
    "                elif key == '纳音': data[\"wuxing_day\"] = value\n",
    "\n",
    "        week_div = soup.find('div', class_='zhong_week')\n",
    "        data[\"weeks\"] = week_div.get_text(strip=True) if week_div else \"\"\n",
    "        qijie_div = soup.find('div', class_='qijie')\n",
    "        if qijie_div: data[\"day\"] = qijie_div.find('a').text.strip() if qijie_div.find('a') else \"\"\n",
    "        \n",
    "        \n",
    "        sucha_div = soup.find('div', class_='sucha')\n",
    "        if sucha_div:\n",
    "            zhoushu_list = sucha_div.find_all('div', class_='zhoushu')\n",
    "            \n",
    "            # 自适应内容解析逻辑\n",
    "            for idx, zhoushu in enumerate(zhoushu_list):\n",
    "                text = zhoushu.get_text(strip=True)\n",
    "                \n",
    "                # 判断是否是节日信息块\n",
    "                if any(keyword in text for keyword in [\"节日\", \"节气\"]):\n",
    "                    # --------- 处理节气信息 ---------\n",
    "                    if \"节气\" in text:\n",
    "                        spans = zhoushu.find_all('span')\n",
    "                        if len(spans) >= 3:\n",
    "                            try:\n",
    "                                # 添加额外格式过滤(\"当前节气()\"转换为\"\")\n",
    "                                data[\"jieqi\"] = re.sub(r'当前节气\\(?([^)]*)\\)?', r'\\1', spans[0].text).strip().strip('（）')\n",
    "                                next_jq = spans[1].text.strip()\n",
    "                                days = spans[2].text.strip()\n",
    "                                data[\"next_jieqi\"] = f\"距离下一个节气{next_jq}还有{days}\"\n",
    "                            except IndexError:\n",
    "                                pass\n",
    "                    # --------- 处理节日信息 ---------\n",
    "                    else:\n",
    "                        try:\n",
    "                            # 增强型节日解析\n",
    "                            if \"是\" in text:\n",
    "                                current_part = re.split(r'是|\\(', text, 1)[1].split(\"距离\")[0]\n",
    "                                festivals = [f.strip(' 、（）') for f in re.split(r'[、,，]', current_part)]\n",
    "                                data[\"festival\"] = \"、\".join(filter(None, festivals))\n",
    "                            else:\n",
    "                                data[\"festival\"] = text.split(\"距离\")[0].strip('（')\n",
    "\n",
    "                            # 下个节日处理逻辑\n",
    "                            next_match = re.search(\n",
    "                                r'距离下一个节日[（\\(]*(.*?)[）\\)]*还有(\\d+)天', \n",
    "                                text\n",
    "                            )\n",
    "                            if next_match:\n",
    "                                data[\"next_festival\"] = f\"距离下一个节日（{next_match.group(1).strip()}）还有{next_match.group(2).strip()}天\"\n",
    "                        except Exception as e:\n",
    "                            print(f\"节日信息解析错误：{str(e)}\")\n",
    "\n",
    "        # 结果后处理清洗\n",
    "        data[\"festival\"] = re.sub(r'[（\\(].*?[）\\)]', '', data[\"festival\"]).strip('，、')\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"副站数据获取失败 {url}: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "def get_shengxiao_info(date_obj, session):\n",
    "    date_str = format_url_date(date_obj)\n",
    "    url = f\"https://m.tthuangli.com/jinrihuangli/xiaoyun_{date_str}.html\"\n",
    "    try:\n",
    "        response = session.get(url, timeout=6)\n",
    "        response.encoding = 'utf-8'\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"生肖吉凶接口请求失败: {str(e)}\")\n",
    "        return {\"good_sx\": \"获取失败\", \"bad_sx\": \"获取失败\"}\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    result = {\"good_sx\": \"\", \"bad_sx\": \"\"}\n",
    "\n",
    "    for container in soup.find_all('div', class_='jibie_tre'):\n",
    "        if container.find('div', class_='teji_desx'):\n",
    "            good_div = container.find('div', class_='sx_info')\n",
    "            if good_div and good_div.span:\n",
    "                result[\"good_sx\"] = good_div.span.get_text().strip()\n",
    "        elif container.find('div', class_='daoshuaisx'):\n",
    "            bad_spans = [\n",
    "                div.span.get_text().strip() \n",
    "                for div in container.find_all('div', class_='shuai_sx_info') \n",
    "                if div.span\n",
    "            ]\n",
    "            result[\"bad_sx\"] = ' '.join(bad_spans)\n",
    "    return result\n",
    "\n",
    "def get_yiji_info(date_str, session):\n",
    "    url = f\"https://m.tthuangli.com/jinrihuangli/yiji_{date_str}.html\"\n",
    "\n",
    "    try:\n",
    "        # 发送HTTPS请求\n",
    "        response = session.get(url, timeout=10) \n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # 编码处理\n",
    "        if response.encoding == 'ISO-8859-1':\n",
    "            response.encoding = response.apparent_encoding\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        def extract_by_css():\n",
    "            container = soup.select_one('div.three_hang table')\n",
    "            ji_numbers = container.select_one('td:nth-of-type(1) .second_color_ji span').text.strip()\n",
    "            auspicious_time = container.select_one('td:nth-of-type(2) .second_color_ji span').text.strip()\n",
    "            return ji_numbers, auspicious_time\n",
    "\n",
    "        def extract_conflict_info(soup):\n",
    "            \"\"\" 解析生肖冲害信息 \"\"\"\n",
    "            # 精确CSS路径定位\n",
    "            conflict_div = soup.select_one('div.hljiexi div.sucha div.chong_sx')\n",
    "            if not conflict_div:\n",
    "                raise ValueError(\"未找到生肖冲突信息\")\n",
    "                \n",
    "            # 获取原始文本并处理\n",
    "            conflict_text = conflict_div.text.strip()\n",
    "            \n",
    "            # 多重清洗保障格式\n",
    "            conflict_text = conflict_text.replace('\\u3000', ' ')  # 替换全角空格\n",
    "            conflict_text = conflict_text.replace('  ', ' ')     # 合并连续空格\n",
    "            return conflict_text\n",
    "\n",
    "        # 数据提取\n",
    "        try:\n",
    "            lucky_numbers, hour_range = extract_by_css()\n",
    "            conflict_text = extract_conflict_info(soup)\n",
    "        except AttributeError as e:\n",
    "            print(f\"元素定位失败：{str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # 数据格式化\n",
    "        formatted_data = {\n",
    "            \"lucky_num\": lucky_numbers.replace(' ', '').replace(',', '、'),\n",
    "            \"noble_time\": hour_range.replace('、', '-').replace('点', '') + '点',\n",
    "            \"conflict_sx\": conflict_text\n",
    "        }\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"网络请求失败：{str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"程序异常：{str(e)}\")\n",
    "\n",
    "    return formatted_data\n",
    "\n",
    "def get_color_info(date_str, session):\n",
    "    url = f\"https://m.tthuangli.com/jinrihuangli/wuxingchuanyi_{date_str}.html\" \n",
    "    try:\n",
    "        response = session.get(url, timeout=10)  # 复用session\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        result = {\n",
    "            'good_color': '',\n",
    "            'bad_color': ''\n",
    "        }\n",
    "\n",
    "        # 处理大吉色\n",
    "        djse = soup.find('div', class_='djse')\n",
    "        if djse:\n",
    "            dj_colors = djse.get_text(strip=True).split('：')[1]\n",
    "            result['good_color'] = dj_colors.replace('、', ' ')  # 直接替换顿号为空格\n",
    "            \n",
    "        # 处理不宜色\n",
    "        byse = soup.find('div', class_='byse')\n",
    "        if byse:\n",
    "            by_colors = byse.get_text(strip=True).split('：')[1]\n",
    "            result['bad_color'] = by_colors.replace('、', ' ')  # 直接替换顿号为空格\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[出现异常] {str(e)}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_lucky_time(date_str, session):\n",
    "    url = f'https://m.tthuangli.com/jinrihuangli/jishi_{date_str}.html'\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        lucky_time_list = []\n",
    "        \n",
    "        for block in soup.find_all('div', class_='jiri_ji'):\n",
    "            title_tag = block.find('div', class_='jiri_ji_tit')\n",
    "            if not title_tag:\n",
    "                continue\n",
    "                \n",
    "            title_parts = title_tag.text.strip().split()\n",
    "            time_name = title_parts[-1] if title_parts else ''\n",
    "            \n",
    "            time_range_tag = block.find('div', class_='juti_time')\n",
    "            time_range = time_range_tag.text.strip() if time_range_tag else ''\n",
    "            \n",
    "            if time_name and time_range:\n",
    "                lucky_time_list.append(f\"{time_name} {time_range}\")\n",
    "\n",
    "        # 修改关键点：删除末尾的 + \", \"\n",
    "        return {\n",
    "            \"lucky_time\": \", \".join(lucky_time_list) if lucky_time_list else \"\"\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)}, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# ====================== 主抓取逻辑 ======================\n",
    "# ...（保持之前的工具函数和抓取函数不变，仅修改主处理结构）...\n",
    "\n",
    "def scrape_single_date(date_str, session):\n",
    "    \"\"\"单个日期的完整抓取流程\"\"\"\n",
    "    main_url = f\"https://www.huangli.net.cn/{date_str}.html\"\n",
    "    \n",
    "    try:\n",
    "        # ===== 主站数据抓取 =====\n",
    "        response = session.get(main_url, timeout=8)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # 解析基础数据\n",
    "        base_data = {\n",
    "            \"datekey\": parse_date_from_url(main_url),\n",
    "            **extract_lunar_info(soup),\n",
    "            **extract_suitable_items(soup),\n",
    "            **parse_god_positions(soup),  # 单独解析财神位\n",
    "            **parse_jixiong_items(soup)    # 解析吉神宜趋和凶煞宜忌\n",
    "        }\n",
    "\n",
    "        # ===== 并发获取副站数据 =====\n",
    "        date_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = {\n",
    "                \"sub\": executor.submit(get_huangli_data, f\"https://m.tthuangli.com/jinrihuangli/{date_str}.html\", session),\n",
    "                \"sx\": executor.submit(get_shengxiao_info, date_obj, session),\n",
    "                \"color\": executor.submit(get_color_info, date_str, session),\n",
    "                \"yiji\": executor.submit(get_yiji_info, date_str, session),\n",
    "                \"lucky_time\": executor.submit(get_lucky_time, date_str, session)\n",
    "            }\n",
    "\n",
    "            # 合并副站数据（自动展开嵌套字段）\n",
    "            for key, future in futures.items():\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    # 特殊处理各个副站的数据结构\n",
    "                    if key == \"sub\":\n",
    "                        base_data.update({\n",
    "                            \"cs\": result.get(\"cs\", \"\"),\n",
    "                            \"constellation\": result.get(\"constellation\", \"\"),\n",
    "                            \"weeks\": result.get(\"weeks\", \"\"),\n",
    "                            \"zs\": result.get(\"zs\", \"\"),\n",
    "                            \"jianxing\": result.get(\"jianxing\", \"\"),\n",
    "                            \"taishen\": result.get(\"taishen\", \"\"),\n",
    "                            \"num_weeks\": result.get(\"num_weeks\", \"\"),\n",
    "                            \"day\": result.get(\"day\", \"\"),\n",
    "                            \"wuxing_day\": result.get(\"wuxing_day\", \"\"),\n",
    "                            \"jieqi\": result.get(\"jieqi\", \"\"),\n",
    "                            \"next_jieqi\": result.get(\"next_jieqi\", \"\"),\n",
    "                            \"festival\": result.get(\"festival\", \"\"),\n",
    "                            \"next_festival\": result.get(\"next_festival\", \"\")\n",
    "                        })\n",
    "                    elif key == \"sx\":\n",
    "                        base_data[\"good_sx\"] = result.get(\"good_sx\", \"\")\n",
    "                        base_data[\"bad_sx\"] = result.get(\"bad_sx\", \"\")\n",
    "                    elif key == \"color\":\n",
    "                        base_data[\"good_color\"] = result.get(\"good_color\", \"\")\n",
    "                        base_data[\"bad_color\"] = result.get(\"bad_color\", \"\")\n",
    "                    elif key == \"yiji\":\n",
    "                        base_data.update({\n",
    "                            \"lucky_num\": result.get(\"lucky_num\", \"\"),\n",
    "                            \"noble_time\": result.get(\"noble_time\", \"\"),\n",
    "                            \"conflict_sx\": result.get(\"conflict_sx\", \"\")\n",
    "                        })\n",
    "                    elif key == \"lucky_time\":\n",
    "                        base_data.update({\n",
    "                            \"lucky_time\": result.get(\"lucky_time\", \"\"),\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"副站 {key} 数据获取失败: {str(e)}\")\n",
    "\n",
    "        return base_data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"主流程异常 {date_str}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 封装公共查找函数\n",
    "def find_item_by_title(title,soup):\n",
    "    for div in soup.find_all('div', class_='item'):\n",
    "        h4 = div.find('h4')\n",
    "        if h4 and h4.text.strip() == title:\n",
    "            return div\n",
    "    return None\n",
    "\n",
    "# ========= 新增的解析函数 =========\n",
    "def parse_god_positions(soup):\n",
    "    \"\"\"解析财神位信息\"\"\"\n",
    "    god_data = {\"godposition\": \"\"}\n",
    "\n",
    "    try:\n",
    "        caishen_div = find_item_by_title('财神位',soup)\n",
    "        if not caishen_div:\n",
    "            raise ValueError(\"未找到财神位区块\")\n",
    "\n",
    "        caishen_data = {}\n",
    "        for li in caishen_div.find('ul').find_all('li'):\n",
    "            key, val = li.text.strip().split('：', 1)\n",
    "            caishen_data[key] = val\n",
    "        \n",
    "        god_data[\"godposition\"] = f\"喜神在{caishen_data['喜神']} 财神在{caishen_data['财神']} 福神在{caishen_data['福神']}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"财神位解析失败: {str(e)}\")\n",
    "    return god_data\n",
    "\n",
    "def parse_jixiong_items(soup):\n",
    "    \"\"\"解析吉神宜趋和凶煞宜忌\"\"\"\n",
    "    jixiong_data = {\"jsyq\": \"\", \"xsyq\": \"\"}\n",
    "    try:\n",
    "        # 吉神宜趋\n",
    "        jsyq_div = soup.find('h4', string='吉神宜趋').find_next('ul', class_='list-2')\n",
    "        jixiong_data[\"jsyq\"] = ' '.join([li.text.strip() for li in jsyq_div.find_all('li')])\n",
    "        \n",
    "        # 凶煞宜忌\n",
    "        xsyq_div = soup.find('h4', string='凶煞宜忌').find_next('ul', class_='list-2')\n",
    "        jixiong_data[\"xsyq\"] = ' '.join([li.text.strip() for li in xsyq_div.find_all('li')])\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"吉凶信息解析失败: {str(e)}\")\n",
    "    return jixiong_data\n",
    "\n",
    "# ====================== 存储模块 ======================\n",
    "class HlDataSaver:\n",
    "    def __init__(self, filename=''):\n",
    "        self.filename = filename\n",
    "\n",
    "    def _datekey_to_datetime(self, item):\n",
    "        \"\"\"统一的日期转换方法（匹配你的具体日期格式）\"\"\"\n",
    "        return datetime.strptime(item[\"datekey\"], \"%Y/%m/%d %H:%M:%S\")\n",
    "\n",
    "    def _sorted_data(self, data_list):\n",
    "        \"\"\"封装统一排序逻辑\"\"\"\n",
    "        return sorted(\n",
    "            data_list,\n",
    "            key=self._datekey_to_datetime,\n",
    "            reverse=False  # 顺序：最旧->最新\n",
    "        )\n",
    "    \n",
    "    def _load_existing(self):\n",
    "        \"\"\"加载现有数据并保持内存数据有效性\"\"\"\n",
    "        if not os.path.exists(self.filename):\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            with open(self.filename, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                # 验证数据格式的有效性\n",
    "                return data.get(\"data\", []) if isinstance(data, dict) else []\n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"加载历史数据失败，重置存储: {str(e)}\")\n",
    "            return []\n",
    "        \n",
    "    def save_incrementally(self, new_data):\n",
    "        \"\"\"增量存储并保持按日期升序排列（线程安全版）\"\"\"\n",
    "        existing = self._load_existing()\n",
    "        existing_dates = {item[\"datekey\"]: idx for idx, item in enumerate(existing)}\n",
    "        modified = False\n",
    "\n",
    "        # 处理更新和新增\n",
    "        for item in new_data:\n",
    "            datekey = item[\"datekey\"]\n",
    "            \n",
    "            # 存在性检查\n",
    "            if datekey in existing_dates:\n",
    "                idx = existing_dates[datekey]\n",
    "                \n",
    "                # 哈希比对避免无意义更新\n",
    "                if hash(frozenset(existing[idx].items())) != hash(frozenset(item.items())):\n",
    "                    existing[idx] = item\n",
    "                    modified = True\n",
    "            else:\n",
    "                # 按顺序插入而不是直接append\n",
    "                insert_pos = next(\n",
    "                    (i for i, x in enumerate(existing) \n",
    "                     if self._datekey_to_datetime(x) > self._datekey_to_datetime(item)),\n",
    "                    len(existing)\n",
    "                )\n",
    "                \n",
    "                existing.insert(insert_pos, item)\n",
    "                existing_dates[datekey] = insert_pos\n",
    "                modified = True\n",
    "\n",
    "        # 双重确认排序逻辑\n",
    "        if modified:\n",
    "            # 最终保存前的全面排序（防止中间插入位置计算错误）\n",
    "            sorted_data = self._sorted_data(existing)\n",
    "            \n",
    "            # 生成统一格式的保存数据\n",
    "            save_data = {\n",
    "                \"version\": 2.0,\n",
    "                \"generated_at\": datetime.now().isoformat(),\n",
    "                \"data\": sorted_data\n",
    "            }\n",
    "\n",
    "            # 原子化保存（避免写文件中途出错破坏数据）\n",
    "            temp_file = f\"{self.filename}.tmp\"\n",
    "            with open(temp_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(save_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            # 替换原文件（跨平台安全操作）\n",
    "            if os.path.exists(self.filename):\n",
    "                os.replace(temp_file, self.filename)\n",
    "            else:\n",
    "                os.rename(temp_file, self.filename)\n",
    "\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# ====================== 增强版数据加载器 ======================\n",
    "class HlDataLoader:\n",
    "    def __init__(self, filename=''):\n",
    "        self.filename = filename\n",
    "        self._data = []\n",
    "        self._index = {}  # datekey到索引的映射\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"加载并预处理数据\"\"\"\n",
    "        if not os.path.exists(self.filename):\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            with open(self.filename, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            # 兼容新旧格式处理\n",
    "            raw_data = data.get('data', [])\n",
    "            if isinstance(raw_data, dict):  # 转换旧格式的字典数据\n",
    "                self._data = list(raw_data.values())\n",
    "            else:\n",
    "                self._data = raw_data\n",
    "                \n",
    "            # 构建索引\n",
    "            self._index = {item['datekey']: idx for idx, item in enumerate(self._data)}\n",
    "        except Exception as e:\n",
    "            logging.error(f\"数据加载失败: {str(e)}\")\n",
    "\n",
    "    def get_by_date(self, date_str):\n",
    "        \"\"\"按日期查询数据（支持多种格式）\"\"\"\n",
    "        target_key = self._normalize_datekey(date_str)\n",
    "        return self._data[self._index.get(target_key, -1)]\n",
    "\n",
    "    def filter_data(self, start_date=None, end_date=None, keywords=None):\n",
    "        \"\"\"高级数据过滤\"\"\"\n",
    "        filtered = []\n",
    "        for item in self._data:\n",
    "            # 日期范围过滤\n",
    "            date_obj = datetime.strptime(item['datekey'], \"%Y/%m/%d %H:%M:%S\")\n",
    "            if start_date and date_obj < start_date:\n",
    "                continue\n",
    "            if end_date and date_obj > end_date:\n",
    "                continue\n",
    "            \n",
    "            # 关键词搜索\n",
    "            if keywords:\n",
    "                search_area = ' '.join(str(v) for v in item.values())\n",
    "                if not any(kw.lower() in search_area.lower() for kw in keywords):\n",
    "                    continue\n",
    "                    \n",
    "            filtered.append(item)\n",
    "        \n",
    "        # 排序保障\n",
    "        filtered.sort(key=lambda x: x['datekey'])\n",
    "        return filtered\n",
    "\n",
    "    def _normalize_datekey(self, date_input):\n",
    "        \"\"\"日期格式统一处理\"\"\"\n",
    "        if isinstance(date_input, datetime):\n",
    "            return date_input.strftime(\"%Y/%m/%d 00:00:00\")\n",
    "            \n",
    "        try:\n",
    "            # 支持多种日期字符串格式\n",
    "            formats = [\n",
    "                \"%Y/%m/%d %H:%M:%S\",\n",
    "                \"%Y-%m-%d\",\n",
    "                \"%Y%m%d\"\n",
    "            ]\n",
    "            for fmt in formats:\n",
    "                try:\n",
    "                    dt = datetime.strptime(date_input, fmt)\n",
    "                    return dt.strftime(\"%Y/%m/%d 00:00:00\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            raise ValueError(\"无法识别的日期格式\")\n",
    "        except:\n",
    "            raise ValueError(\"无效的日期输入\")\n",
    "        \n",
    "    # 统计分析功能（示例）\n",
    "    def get_statistics(self):\n",
    "        \"\"\"获取黄历数据统计信息\"\"\"\n",
    "        stats = {\n",
    "            'total_days': len(self._data),\n",
    "            'most_common_jsyq': Counter(item['jsyq'] for item in self._data).most_common(5),\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "    # 数据完整性检测（示例）\n",
    "    def validate_data(self):\n",
    "        \"\"\"校验数据完整性\"\"\"\n",
    "        missing_fields = []\n",
    "        for idx, item in enumerate(self._data):\n",
    "            if 'datekey' not in item:\n",
    "                missing_fields.append(f\"索引 {idx} 缺少datekey字段\")\n",
    "                continue\n",
    "                \n",
    "            # 校验日期格式合法性\n",
    "            try:\n",
    "                datetime.strptime(item['datekey'], \"%Y/%m/%d %H:%M:%S\")\n",
    "            except:\n",
    "                missing_fields.append(f\"索引 {idx} datekey格式错误: {item['datekey']}\")\n",
    "                \n",
    "        return missing_fields\n",
    "    \n",
    "    def save_data(self, output_file=None):\n",
    "        \"\"\"保存数据并自动排序，覆盖重复日期\"\"\"\n",
    "        try:\n",
    "            target_file = output_file or self.filename\n",
    "            \n",
    "            # 转换字典去重并保留最后出现的日期数据\n",
    "            data_dict = {}\n",
    "            duplicate_count = 0\n",
    "            for item in self._data:\n",
    "                datekey = item.get('datekey')\n",
    "                if datekey:\n",
    "                    if datekey in data_dict:\n",
    "                        duplicate_count += 1\n",
    "                    data_dict[datekey] = item\n",
    "\n",
    "            # 排序处理\n",
    "            sorted_data = sorted(data_dict.values(), key=self._datekey_to_datetime)\n",
    "            \n",
    "            # 构建保存数据结构\n",
    "            save_data = {\n",
    "                \"version\": 1.2,\n",
    "                \"generated_at\": datetime.now().isoformat(),\n",
    "                \"data\": sorted_data\n",
    "            }\n",
    "\n",
    "            # 写入文件\n",
    "            with open(target_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(save_data, f, indent=2, ensure_ascii=False)\n",
    "                \n",
    "        except PermissionError:\n",
    "            print(\"错误：没有文件写入权限\")\n",
    "        except Exception as e:\n",
    "            print(f\"保存失败: {str(e)}\")\n",
    "\n",
    "    def _datekey_to_datetime(self, item):\n",
    "        \"\"\"内部日期转换方法\"\"\"\n",
    "        return datetime.strptime(item['datekey'], \"%Y/%m/%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "# ====================== 执行入口 ======================\n",
    "def main(start_date, end_date):\n",
    "    \"\"\"主控制函数\"\"\"\n",
    "    # 生成日期列表\n",
    "    date_objs = [start_date + timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n",
    "    target_dates = [format_url_date(d) for d in date_objs]\n",
    "\n",
    "    # 创建共享会话\n",
    "    session = create_session()\n",
    "\n",
    "    # 使用线程池并发处理日期\n",
    "    # 存储优化：逐条处理+实时保存\n",
    "    saver = HlDataSaver(filename='huangli_data.json')\n",
    "    loader = HlDataLoader(filename='huangli_data.json')\n",
    "\n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        futures = {executor.submit(scrape_single_date, date_str, session): date_str \n",
    "                  for date_str in target_dates}\n",
    "        \n",
    "        # 实时处理完成结果\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            date_str = futures[future]\n",
    "            try:\n",
    "                if data := future.result():\n",
    "                    # 实时保存单日数据\n",
    "                    saver.save_incrementally([data])\n",
    "                    # loader._data.append(data)\n",
    "                    # loader.save_data()\n",
    "\n",
    "                    results.append(data)\n",
    "                    logging.info(f\"√ {date_str} 处理并保存完成\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"× {date_str} 处理失败: {str(e)}\")\n",
    "\n",
    "    return {\n",
    "        \"status\": 0,\n",
    "        \"saved_days\": len(results),\n",
    "        \"failed_days\": len(target_dates)-len(results)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 17:32:44,217 [INFO] √ 2025-3-1 处理并保存完成\n",
      "2025-02-28 17:32:44,221 [INFO] √ 2025-3-2 处理并保存完成\n",
      "2025-02-28 17:32:44,223 [INFO] √ 2025-3-3 处理并保存完成\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "操作完成 | 成功: 3 天 | 失败: 0 天\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 配置抓取日期范围\n",
    "    start_date = datetime(2025, 3, 1)\n",
    "    end_date = datetime(2025, 3, 3)\n",
    "    \n",
    "    # 执行抓取\n",
    "    output = main(start_date, end_date)\n",
    "    \n",
    "    # 输出结果\n",
    "    print(f\"操作完成 | 成功: {output['saved_days']} 天 | 失败: {output['failed_days']} 天\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 使用示例 ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始化加载器\n",
    "    loader = HlDataLoader(filename='huangli_data.json')\n",
    "    \n",
    "    # 查询单个日期（多种格式支持）\n",
    "    print(loader.get_by_date(\"2025-03-02\"))  # 标准日期格式    \n",
    "    \n",
    "    # 复杂查询：2025年3月包含\"星期日\"的日子\n",
    "    results = loader.filter_data(\n",
    "        start_date=datetime(2025,3,1),\n",
    "        end_date=datetime(2025,3,31),\n",
    "        keywords=[\"星期日\"]\n",
    "    )\n",
    "    \n",
    "    # 显示查询结果\n",
    "    for item in results:\n",
    "        print(f\"{item['datekey']}: {item.get('lucky_num')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = loader.get_statistics()\n",
    "\n",
    "# 输出统计结果\n",
    "print(f\"总天数: {stats['total_days']}\")\n",
    "print(\"\\n常见吉神趋日Top5:\")\n",
    "for item, count in stats['most_common_jsyq']:\n",
    "    print(f\"{item}: {count} 次\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = loader.validate_data()\n",
    "\n",
    "# 根据结果处理\n",
    "if not errors:\n",
    "    print(\"🌟 数据效验通过，无异常\")\n",
    "else:\n",
    "    print(f\"发现 {len(errors)} 项问题:\")\n",
    "    for error in errors:\n",
    "        print(f\"❌ {error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = HlDataLoader(filename='huangli_data.json')\n",
    "\n",
    "# 获取统计信息\n",
    "stats = loader.get_statistics()\n",
    "\n",
    "# 验证数据\n",
    "errors = loader.validate_data()\n",
    "\n",
    "# 生成报告\n",
    "report = {\n",
    "    \"data_source\": loader.filename,\n",
    "    \"days_covered\": f\"{stats['total_days']} 天\",\n",
    "    \"data_quality\": \"PASS\" if not errors else f\"{len(errors)} errors\",\n",
    "    \"recommendations\": [\n",
    "        \"定期备份数据\",\n",
    "        \"异常日期需要人工复核\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(json.dumps(report, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功保存145条数据\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = \"https://jinpaper.com/pages/copy-of-2022-%E6%AF%8F%E6%9C%88%E8%AF%9E%E8%BE%B0\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "data = []\n",
    "current_month = None\n",
    "remain_rows = 0  # rowspan剩余行数\n",
    "\n",
    "for tr in soup.select('table[dir=\"ltr\"] tr')[2:]:  # 跳过前两行表头\n",
    "    tds = tr.find_all('td')\n",
    "    \n",
    "    # 检测月份行：带rowspan属性的单元格\n",
    "    month_td = next((td for td in tds if td.get('rowspan') and '月' in td.text), None)\n",
    "    \n",
    "    if month_td:\n",
    "        # 处理新月份行\n",
    "        current_month = month_td.get_text(strip=True).replace(\"）\", \"\").replace(\"（\", \"-\")\n",
    "        remain_rows = int(month_td['rowspan']) - 1  # 算上当前行\n",
    "        \n",
    "        # 提取当前行数据\n",
    "        other_tds = [td for td in tds if td != month_td]\n",
    "        if len(other_tds) >= 3:\n",
    "            solar = other_tds[0].get_text(strip=True).replace(\"号\", \"日\")\n",
    "            lunar = other_tds[1].get_text(strip=True)\n",
    "            festival = other_tds[2].get_text(strip=True)\n",
    "            \n",
    "            data.append({\n",
    "                \"date\": solar,\n",
    "                \"nongli\": lunar,\n",
    "                \"festival\": festival\n",
    "            })\n",
    "    else:\n",
    "        # 处理普通数据行\n",
    "        if remain_rows > 0 and len(tds) >= 3:\n",
    "            solar = tds[0].get_text(strip=True).replace(\"号\", \"日\")\n",
    "            lunar = tds[1].get_text(strip=True)\n",
    "            festival = tds[2].get_text(strip=True)\n",
    "            \n",
    "            data.append({\n",
    "                \"date\": solar,\n",
    "                \"nongli\": lunar,\n",
    "                \"festival\": festival\n",
    "            })\n",
    "            remain_rows -= 1\n",
    "\n",
    "# 保存为JSON\n",
    "with open('2022_correct_festivals.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2, separators=(',', ': '))\n",
    "\n",
    "print(f\"成功保存{len(data)}条数据\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
